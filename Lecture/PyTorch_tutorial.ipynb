{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Quickstart Tutorial\n",
    "\n",
    "<i>This tutorial is to introduce the basics of using PyTorch.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Working with data](#working-with-data)\n",
    "2. [Creating Models](#creating-models)\n",
    "3. [Optimizing the Model Parameters](optimzing-the-model-parameters)\n",
    "4. [Saving Models](#saving-models)\n",
    "5. [Loading Models](#loading-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data\n",
    "- [Dataset](#dataset)\n",
    "- [DataLoader](#dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "#### What is `Dataset`?\n",
    "- Storage of the samples and their corresponding labels\n",
    "- `torchvision.datasets` module contains `Dataset` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "\n",
    "### FashionMNIST: Dataset with 60000, 28x18 grayscale images in 10 categories ###\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),  ## \"transform\" the data to \"tensor\" format\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for better understanding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "### We can see that the number of training data is 60000 ###\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "### We can see what's in the training_data: tuple with tensor and label ###\n",
    "return_something = training_data[0]\n",
    "# print(return_something)\n",
    "print(type(return_something))  ## this 'tuple': (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "### ToTensor(): convert np.ndarray ---> torch.FlaotTensor ###\n",
    "    # It converts\n",
    "    # PIL Image or np.ndarray -----------> torch.FloatTensor\n",
    "    #   in the range [0, 255]           in the range [0.0, 1.0]\n",
    "    #       (H x W x C)                       (C x H x W)\n",
    "    # (Height x Width x Channel)      (Channel x Height x Width)\n",
    "\n",
    "    # if the PIL Image belongs to one of the models (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
    "    # or if the numpy.ndarray has dtype = np.uint8\n",
    "\n",
    "# Check a data (index: 0)\n",
    "img, label = training_data[0]\n",
    "print(img.shape)  # torch.Size([channel, height, width])\n",
    "print(label)  # this image is labeled to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example for `Dataset`\n",
    "- Every TorchVision `Dataset` includes two arguments to modify the samples & labels respectively:\n",
    "    - `transform`\n",
    "    - `target_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from torchvision.io import read_image\n",
    "\n",
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "#         self.img_labels = pd.read_csv(annotations_file)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "#### What is `DataLoader`?\n",
    "- Wraps an iterable around the `Dataset`\n",
    "- Supports automatic batching, sampling, shuffling, and multiprocess data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Define a batch size of 64\n",
    "batch_size = 64  ## each element in the dataloader iterable will return a batch of 64 features & labels\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# How dataloader work\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "# Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28]) => meaning: dataloader now want to run 64 times and stack everything to new exit\n",
    "# (C, H, W) ---------> (B, C, H, W)\n",
    "#           (Batch_size, Channel, Height, Width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For better understanding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len DataLoader: 157\n",
      "Len DataLoader: 938\n"
     ]
    }
   ],
   "source": [
    "print(f\"Len DataLoader: {len(test_dataloader)}\")\n",
    "print(f\"Len DataLoader: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000 / 64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
